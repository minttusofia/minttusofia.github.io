<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Minttu Alakuijala</title>
  
  <meta name="author" content="Minttu Alakuijala">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/x-icon" href="images/ur5_pick_square.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Minttu Alakuijala</name>
              </p>
	      <p>I am a Postdoctoral Researcher at <a href="https://aalto.fi/en">Aalto University</a> working on <b>language-conditioned reinforcement learning</b> (RL), and most recently, <b>systematic thinking in LLMs</b>. I am working with <a href="https://people.aalto.fi/samuel.kaski">Samuel Kaski</a> and <a href="https://users.ics.aalto.fi/pemartti/">Pekka Marttinen</a>, and I coordinate the <a href="https://fcai.fi/fcai-teams#foundation">Foundation models for language & reinforcement learning team</a> at <a href="https://fcai.fi">FCAI</a>, the Finnish Center for Artificial Intelligence.
              </p>
              <p>

	      Previously, I obtained my PhD from <a href="https://ai.google/research">Google Research</a> (through the CIFRE scheme), <a href="https://www.inria.fr/en">Inria</a> and <a href="https://www.ens.psl.eu/">Ecole Normale Superieure</a> in Paris. My PhD research focused on RL and learning from demonstration for robotic manipulation tasks.
                I defended my <a href="https://hal.science/tel-04001370">thesis</a> on <i>Autonomous and Weakly-Supervised Learning for Robotic Manipulation</i> in December 2022 and I was advised by <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>, <a href="https://www.di.ens.fr/~ponce/">Jean Ponce</a> and <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>. At Inria, I was a member of the <a href="https://www.di.ens.fr/willow/">Willow</a> and <a href="https://team.inria.fr/thoth/">Thoth</a> teams.
              </p>
              <p style="text-align:center">
                <a href="mailto:<firstname>.<lastname>@gmail.com">Email</a> &nbsp|&nbsp
                <a href="data/cv_minttu_alakuijala.pdf">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=WmrdmjIAAAAJ">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/minttualakuijala/"><img class="social" src="images/icons/linkedin.png" alt="LinkedIn"/></a> &nbsp|&nbsp
                <a href="https://twitter.com/malakuijala"><img class="social" src="images/icons/twitter.png" alt="Twitter"/></a> &nbsp|&nbsp
                <a href="https://github.com/minttusofia/"><img class="social" src="images/icons/github.png" alt="Github"/></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a><img style="width:100%;max-width:100%" alt="profile photo" src="images/MinttuAlakuijala_square.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
	      There are considerable mutual benefits to be gained from integrating ideas from reinforcement learning (RL) and language models. LLMs possess a real-world knowledge base that is vast but mostly static, and not specific enough for many environments. Moreover, decoding is typically greedy, without planning or lookahead. RL, on the other hand, formalizes learning from interaction, trading off exploration and short-term reward maximization, and planning with the future in mind.

	      <p>
	      My objective is twofold: I aim to <b>equip RL agents with world knowledge and transferable skills that models like LLMs and VLMs have learned</b> from internet-scale text and vision corpora. Furthermore, I am looking to <b>teach LLMs to think and plan more explicitly</b> – using utilities such as task decomposition and code – <b>and to learn from embodied interaction</b> like RL agents.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vlc_teaser.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.19988">
                <papertitle>Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics</papertitle>
              </a>
              <br>
              <strong>Minttu Alakuijala</strong>,
              <a href="https://www.reggiemclean.ca/">Reginald McLean</a>,
              <a href="https://cs.torontomu.ca/~iwoungan/">Isaac Woungang</a>,
              <a href="http://narimanfarsad.com/">Nariman Farsad</a>,
              <a href="https://kaski-lab.com/">Samuel Kaski</a>,
              <a href="https://users.ics.aalto.fi/~pemartti/">Pekka Marttinen</a>,
              <a href="https://scholar.google.com/citations?user=8eLlbhMAAAAJ&hl=en">Kai Yuan</a>
              <br>
              <em>TMLR, 2025</em> 
              <br>
              <a href="https://arxiv.org/abs/2405.19988">arXiv</a>
              <p></p>
              <p>
               We train language-conditioned robotic reward functions from actor-agnostic data, and outperform 5 existing methods with a novel combination of contrastive and sequential ranking objectives, together ensuring both smooth and accurate rewards. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/cwm_teaser.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.15383">
                <papertitle>Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=tXmPE94AAAAJ&hl=en">Nicola Dainese</a>,
              <a href="https://scholar.google.com/citations?user=2_dW7y4AAAAJ&hl=en">Matteo Merler</a>,
              <strong>Minttu Alakuijala</strong>,
              <a href="https://users.ics.aalto.fi/~pemartti/">Pekka Marttinen</a>
              <br>
              <em>NeurIPS 2024</em> 
              <br>
              <a href="https://arxiv.org/abs/2405.15383">arXiv</a>
              <p></p>
              <p>
                We propose to model RL environments with code written by an LLM, propose a method to improve code generation for this task and show how to plan with code world models.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
	        <img src='images/RDD.png' width="170", style="position: relative; top: 40px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=MZG5VzXBm9">
                <papertitle>Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning</papertitle>
              </a>
              <br>
              <a href="https://sergiohg.com/">Sergio Hernandez-Gutierrez</a>,
              <strong>Minttu Alakuijala</strong>,
              <a href="https://anikitin.me/">Alexander Nikitin</a>,
              <a href="https://users.ics.aalto.fi/~pemartti/">Pekka Marttinen</a>
              <br>
              <em>NeurIPS 2024 Workshop on System 2 Reasoning at Scale</em> 
              <br>
              <a href="https://openreview.net/forum?id=MZG5VzXBm9">Paper</a>
              <p></p>
              <p>
	        We present a novel divide-and-conquer method to solve reasoning problems with LLMs, employing recursive decomposition with dependency modeling in generic settings.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/icra2023_teaser.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.09019">
                <papertitle>Learning Reward Functions for Robotic Manipulation by Observing Humans</papertitle>
              </a>
              <br>
              <strong>Minttu Alakuijala</strong>,
              <a href="http://gabe.squirrelsoup.net/">Gabriel Dulac-Arnold</a>,
              <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>,
              <a href="https://www.di.ens.fr/~ponce/">Jean Ponce</a>,
              <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>
              <br>
              <em>ICRA</em>, 2023
              <br>
              <a href="https://sites.google.com/view/hold-rewards">project page</a>
              |
              <a href="https://arxiv.org/abs/2211.09019">arXiv</a>
              <p></p>
              <p>
                We learn dense reward functions for robotic manipulation by learning about distances in state space from videos of humans only, using self-supervised contrastive and regression objectives.
              </p>
            </td>
          </tr>		
		  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/rrlfd_teaser.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2106.08050">
                <papertitle>Residual Reinforcement Learning from Demonstrations</papertitle>
              </a>
              <br>
              <strong>Minttu Alakuijala</strong>,
              <a href="http://gabe.squirrelsoup.net/">Gabriel Dulac-Arnold</a>,
              <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>,
              <a href="https://www.di.ens.fr/~ponce/">Jean Ponce</a>,
              <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>
              <br>
              <em>RSS 2020 Workshop on Advances & Challenges in Imitation Learning for Robotics</em>
              <br>
              <a href="https://sites.google.com/view/rrlfd">project page</a>
              |
              <a href="https://arxiv.org/abs/2106.08050">arXiv</a>
              <p></p>
              <p>
                Starting from a small number of task demonstrations on a robot arm, we learn an initial base policy and a task-relevant, low-dimensional representation space through behavioral cloning, which is then autonomously improved through residual reinforcement learning using images, proprioceptive inputs and sparse rewards only.
              </p>
            </td>
          </tr>		
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/narrated_actions_teaser.png' width="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/18FfUR6R2EM5YWhUWiYEO4LcFbzGEz8on/view">
                <papertitle>Discovering Actions by Jointly Clustering Video and Narration Streams Across Tasks</papertitle>
              </a>
              <br>
              <strong>Minttu Alakuijala</strong>,
              <a href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>,
              <a href="https://www.di.ens.fr/~ponce/">Jean Ponce</a>,
              <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>
              <br>
              <em>CVPR 2020 Workshop on Learning from Instructional Videos</em>
              <br>
              <a href="https://drive.google.com/file/d/18FfUR6R2EM5YWhUWiYEO4LcFbzGEz8on/view">video</a>
              |
              <a href="data/CVPR2020_WLIV_Poster.pdf">poster</a>
              <p></p>
              <p>
                Using only weak supervision from the timing of narration and visual information, we segment narrated tutorial videos into <i>k</i> action classes or background. We use a discriminative clustering objective together with an inconsistency penalty which encourages the timing and order of actions in the visual stream to match that of the narration stream in each video.
              </p>
            </td>
          </tr>		
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
